%% LaTeX2e class for student theses
%% sections/abstract_de.tex
%% 
%% Karlsruhe Institute of Technology
%% Institute for Program Structures and Data Organization
%% Chair for Software Design and Quality (SDQ)
%%
%% Dr.-Ing. Erik Burger
%% burger@kit.edu
%%
%% Version 1.3.6, 2022-09-28

\Abstract
Die Automatisierung von Testprozessen ist ein wichtiger Schritt zur Verbesserung der Softwarequalität und zur Minimierung von Fehlern.
Automatisierte Tests können Zeit und Kosten sparen, indem sie eine schnelle und effiziente Identifizierung und Behebung von Fehlern ermöglichen.
Die Implementierung und Wartung solcher Tests, die ausschließlich auf die grafischen Benutzeroberfläche zugreifen, kann jedoch zeitaufwändig und teuer sein.
Wünschenswert wären automatisierte Tests, die bei Änderungen an der Benutzeroberfläche stabil bleiben und Fehler ohne hartkodierte Fehlerbedingungen erkennen.
Ein vielversprechender Ansatz ist die Verwendung von großen Sprachmodellen (LLM) zur Generierung von Benutzeraktionen.
Diese Arbeit testet dies in einem realistischen Szenario und untersucht, wie effektiv das LLM darin ist die Menge der Anwendungszustände zu erkunden und Fehlerzustände zu erkennen.