%% LaTeX2e class for student theses
%% sections/conclusion.tex
%% 
%% Karlsruhe Institute of Technology
%% Institute for Program Structures and Data Organization
%% Chair for Software Design and Quality (SDQ)
%%
%% Dr.-Ing. Erik Burger
%% burger@kit.edu
%%
%% Version 1.3.6, 2022-09-28

\chapter{Ergebnisse}
\label{ch:Results}

As we know of no other approaches to doing scripted testing using only natural language test descriptions, comparing it to other works is hard.
As such validation of this part of the work will center around its feasibility.
The new system's ability to navigate the AUT will be compared to monkey testing.
The test oracle part of the system will be validated using mutation testing and its results will be captured in a confusion matrix.

\section{Vergleich zu Monkey Testing}

Monkey testing is the approach of taking random user actions to find failing application states.
We will measure if the system can successfully navigate the software into the state that shall be tested for each test case and count the number of user interactions it simulates.
That amount of user interaction will be compared to the number of user interactions performed by a monkey tester to navigate to the same state.
The system must at least use fewer user interactions as a monkey tester.

\section{Mutationstests}

For each test instance, first, an error-free version of the demo application will be tested using the new system.
In this case, it is expected to classify all test runs as 'pass' and a classification as 'failure' is a false negative.
Then a different version of the demo application will be tested.
This version will include an error that manifests if the test description is correctly followed.
Testing that version the new system is expected to classify the test run as a 'failure' and a classification as 'pass' is a false positive.

The artifact of this verification step is a confusion matrix, a contingency table that is common in the field of statistical classification.