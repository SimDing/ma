%% LaTeX2e class for student theses
%% sections/conclusion.tex
%% 
%% Karlsruhe Institute of Technology
%% Institute for Program Structures and Data Organization
%% Chair for Software Design and Quality (SDQ)
%%
%% Dr.-Ing. Erik Burger
%% burger@kit.edu
%%
%% Version 1.3.6, 2022-09-28



\newcommand{\minipicture}[3]{
    \begin{tikzpicture}
        \begin{axis}[
            ymajorgrids=true,
            xmin=0, xmax=#3,
            grid style=dashed,
            ylabel={Abdeckung [\%]},
            xlabel={Interaktionen [1]},
            width=#2,
            no markers,
            ]
            
            \addplot[color=green] table [x=x,y=average,col sep=comma] {experimental_data/monkey/results.csv};
            \addplot+[forget plot,name path=monkey_top,color=green!70] table [x=x,y=above,col sep=comma] {experimental_data/monkey/results.csv};
            \addplot+[forget plot,name path=monkey_bottom,color=green!70] table [x=x,y=below,col sep=comma] {experimental_data/monkey/results.csv};
            \addplot+[forget plot,green!50,fill opacity=0.5] fill between[of=monkey_top and monkey_bottom];

            \addplot[color=black] table [x=x,y=average,col sep=comma] {experimental_data/#1/results.csv};
            \addplot+[forget plot,name path=#1_top,color=black!70] table [x=x,y=above,col sep=comma] {experimental_data/#1/results.csv};
            \addplot+[forget plot,name path=#1_bottom,color=black!70] table [x=x,y=below,col sep=comma] {experimental_data/#1/results.csv};
            \addplot+[forget plot,black!50,fill opacity=0.5] fill between[of=#1_top and #1_bottom];
        \end{axis}
    \end{tikzpicture}
}

\newcommand{\miniplot}[4]{
    \begin{subfigure}[b]{0.47\textwidth}
        \minipicture{#1}{\textwidth}{#4}
        \caption{#2}
        \label{fig:#3}
    \end{subfigure}
}

\chapter{Ergebnisse}
\label{ch:Results}

\begin{figure}
    % 25 interactions
    \centering
    \begin{tabular}{|l|c|c|c|c|c|c|}
        \hline
        Modell & Eingabe & n & \multicolumn{4}{c|}{Branchenabdeckung [\%]}  \\
         &  &  & Mittel & Min & Max & $\sigma$ \\
        \hline
        Monkey & - & 10 & 71,2 & 60,2 & 78,6 & 4,8 \\
        GPT-3.5 & Verkettung & 4 & 65,8 & 56,1 & 70,4 & 5,7 \\
        GPT-3.5 & Beschreibung & 4 & 67,9 & 58,2 & 75,5 & 6,3 \\
        GPT-3.5 & Erklärung & 4 & 60,2 & 54,1 & 67,3 & 5,1 \\
        GPT-3.5 & Ziele & 5 & 61,8 & 52 & 68,4 & 6,3 \\
        GPT-3.5 & Chain of Thought & 4 & 68,6 & 57,1 & 74,5 & 6,9 \\
        GPT-4 & Verkettung & 2 & 74,0 & 73,5 & 74,5 & 0,5 \\
        GPT-4 & Beschreibung & 3 & 66,0 & 62,2 & 73,5 & 5,3 \\
        GPT-4 & Erklärung & 5 & 77,6 & 74,5 & 79,6 & 2,1 \\
        GPT-4 & Ziele & 4 & 79,1 & 75,5 & 83,7 & 3,3 \\
        GPT-4 & Chain of Thought & 4 & 81,4 & 78,6 & 87,8 & 3,8 \\
        \hline
    \end{tabular}
    \caption{Zusammenfassung der Ergebnisse der Tests mit den Sprachmodellen und Monkey-Testing nach 25 Interaktionen.}
    \label{tab:results}
\end{figure}



\section{Erste Tests mit GPT-3.5 und der ursprünglichen Eingabe}

Nach der Implementierung des Systems wurden zuerst Tests mit GPT-3.5 durchgeführt, um die Funktionalität des Systems zu überprüfen; Übersicht der Ergebnisse siehe Abbildung \ref{tab:results}.
Dabei wurde eine Eingabe verwendet, die der von \Citeauthor{GPT3Testing} nachempfunden ist:
Darin wird das LLM aufgefordert, zuerst eine Benutzerinteraktion zu generieren und dann eine Begründung zu liefern, warum diese Benutzerinteraktion generiert wurde.
Die gleiche Eingabe lässt sich nicht verwenden, da in meiner Implementierung mehr Arten von Benutzerinteraktionen unterstützt werden und die Ausgabe anders aufgebaut sein muss.
Ergebnisse von Tests mit dieser Eingabe sind in Abbildung \ref{fig:gpt3_5_explain_after} zu sehen.
Mit dieser Eingabe war die Branchenabdeckung nach 25 Interaktionen mit durchschnittlich $55{,}7\%$ deutlich niedriger als die von Monkey-Testing mit durchschnittlich $71{,}2\%$.

Ein Problem, das bei diesen Tests auftrat, war, dass das LLM oft identische Benutzerinteraktionen mehrfach in Folge generierte.
Beispielsweise wurde mehrmals hintereinander mit jeweils identischer Begründung auf die Schaltfläche \enquote{Submit} geklickt.
Angesichts dessen, dass GPT-3.5 in anderen Anwendungsgebieten gute Ergebnisse erzielt, war das Ergebnis enttäuschend und ich entschied, Eingaben zu suchen, die zu besseren Ergebnissen führen könnten.
Die Resultate dieser Suche sind die in Abschnitt \ref{sec:prompt-engineering} beschriebenen Eingaben.

\begin{figure}[h]
    \centering
    \minipicture{gpt3-5-explain-after}{0.47\textwidth}{25}
    \caption{Ergebnisse der Tests mit GPT-3.5 und einer Eingabe, die verlangt erst eine Benutzerinteraktion und dann eine Begründung zu generieren. Die schwarze Linie zeigt die durchschnittliche Branchenabdeckung, die grüne Linie zeigt die durchschnittliche Branchenabdeckung von Monkey-Testing. Der schattierte Bereich um die Linien zeigt die Standardabweichung.}
    \label{fig:gpt3_5_explain_after}
\end{figure}

\section{Suche nach besseren Eingaben für GPT-3.5}
\label{sec:prompt-engineering:2}


\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            ymajorgrids=true,
            xmin=0, xmax=25,
            grid style=dashed,
            width=0.9\textwidth,
            ylabel={durchschnittliche Branchenabdeckung [\%]},
            xlabel={Anzahl der generierten Benutzerinteraktionen [1]},
            no markers,
            legend pos=south east,
            %xtick=data,
            ]
            \addplot[color=green] table [x=x,y=average,col sep=comma] {experimental_data/monkey/results.csv};
            \addlegendentry{Monkey-Testing, n=10}

            \addplot[color=black] table [x=x,y=average,col sep=comma] {experimental_data/gpt3-5-base/results.csv};
            \addlegendentry{GPT-3.5 Basiseingabe, n=1}

            \addplot[color=red] table [x=x,y=average,col sep=comma] {experimental_data/gpt3-5-chained/results.csv};
            \addlegendentry{GPT-3.5 Verkettung, n=4}

            \addplot[color=blue] table [x=x,y=average,col sep=comma] {experimental_data/gpt3-5-description/results.csv};
            \addlegendentry{GPT-3.5 Beschreibung, n=4}

            \addplot[color=cyan] table [x=x,y=average,col sep=comma] {experimental_data/gpt3-5-explain/results.csv};
            \addlegendentry{GPT-3.5 Erklärung, n=4}

            \addplot[color=orange] table [x=x,y=average,col sep=comma] {experimental_data/gpt3-5-goals/results.csv};
            \addlegendentry{GPT-3.5 Ziele, n=5}

            \addplot[color=magenta] table [x=x,y=average,col sep=comma] {experimental_data/gpt3-5-chain-of-thought/results.csv};
            \addlegendentry{GPT-3.5 Chain of Thought, n=4}
        \end{axis}
    \end{tikzpicture}
    \caption{Vergleich der durchschnittlich gemessenen Branchenabdeckung von GPT-3.5 mit allen Eingaben und Monkey-Testing.}
    \label{fig:gpt3_5_all}
\end{figure}

\begin{figure}
    \centering
    \miniplot{gpt3-5-base}{GPT-3.5 Basiseingabe, n=1}{gpt3_5_base}{25}\hfill
    \miniplot{gpt3-5-chained}{GPT-3.5 Verkettung, n=4}{gpt3_5_chained}{25}
    \miniplot{gpt3-5-description}{GPT-3.5 Beschreibung, n=4}{gpt3_5_describe}{25}\hfill
    \miniplot{gpt3-5-explain}{GPT-3.5 Erklärung, n=4}{gpt3_5_explain}{25}
    \miniplot{gpt3-5-goals}{GPT-3.5 Ziele, n=5}{gpt3_5_goals}{25}\hfill
    \miniplot{gpt3-5-chain-of-thought}{GPT-3.5 Chain of Thought, n=4}{gpt3_5_chain_of_thought}{25}
    \caption{Vergleich der durchschnittlich gemessenen Branchenabdeckung von GPT-3.5 (schwarz) und Monkey-Testing (grün). Der schattierte Bereich um die Linien zeigt die Standardabweichung.}
\end{figure}

Die erste Eingabe, die ich ausprobierte, war die Basiseingabe, bei der dem LLM nicht die Ein- und Ausgaben vorheriger Interaktionen gegeben werden.
Das führt zu noch schlechteren Ergebnissen, wie in Abbildungen \ref{fig:gpt3_5_all} und \ref{fig:gpt3_5_base} zu sehen ist.
Der LLM-Tester führt bei gleichem Zustand fast immer die gleiche Aktion aus, was dazu führt, dass er in Schleifen von ein bis zwei Interaktionen stecken bleibt und keine neuen Aktionen ausführt.

Die nächste Eingabe, die ich ausprobierte, war die Verkettung, bei der dem LLM die Ein- und Ausgaben vorheriger Interaktionen mit in die Eingabe gegeben werden.
Die Ergebnisse dieser Eingabe sind in Abbildung \ref{fig:gpt3_5_chained} zu sehen.
Mit dieser Eingabe war die Branchenabdeckung nach 25 Interaktionen schon $65{,}8\%$, auch wenn der Unterschied zur ursprünglichen Eingabe nur das Fehlen der Forderung nach einer Begründung ist.

Fordert man vor der Benutzerinteraktion eine Beschreibung der Webseite, steigt die Branchenabdeckung auf $67{,}9\%$ und es scheint, als ob nicht mehr viel fehlen würde, um die Branchenabdeckung von Monkey-Testing zu erreichen.
Die nächste Ergänzung der Eingabe, das Fordern einer Erklärung, was mit der Benutzerinteraktion erreicht werden soll, führte aber wieder zu einer deutlichen Verschlechterung der Ergebnisse, wie in Abbildung \ref{fig:gpt3_5_explain} zu sehen ist.

Ein Hauptproblem mit dieser Eingabe scheint zu sein, dass das Sprachmodell die Webseite nicht versteht und deshalb nicht in der Lage ist, sinnvolle Handlungsabläufe zu generieren:
Beispielsweise wurde immer wieder die Erklärung \enquote{\foreignlanguage{english}{Proceed with the checkout process by submitting the payment information.}} (Deutsch: \enquote{Fahre mit dem Bezahlvorgang fort, indem die Zahlungsinformationen abgeschickt werden.}) generiert, obwohl die Zahlungsinformationen nicht gesondert vom Rest des Bestellvorgangs abgeschickt werden können.
Ferner beachtete das Sprachmodell auch die daraufhin auf der Webseite erscheinende Fehlermeldung nicht und versuchte immer wieder das halb ausgefüllte Formular abzuschicken.

In einem anderen Test versucht das Sprachmodell immer wieder den \enquote{Pfeil nach oben}-Knopf im Warenkorb (Abbildung \ref{fig:online-shopping-website-cart}) zu drücken, mit dem die Menge eines Artikels erhöht werden kann, um eine nähere Beschreibung dieses Artikels zu erhalten.
Das kommt vermutlich daher, dass die textuelle Repräsentation dieses Knopfes den Schriftzug \enquote{fa-cart-plus} enthält, was das Sprachmodell scheinbar als Indiz dafür interpretiert, dass es sich um einen Knopf für eine Detailansicht handelt.
(Danach folgt \enquote{fa-cart-minus}, somit ist diese Interpretation unwahrscheinlich.)

Ein anderes Problem ist, dass in einigen Fällen die erzeugte Handlungsabsicht nicht in einer Benutzerinteraktion ausführbar ist, beispielsweise wurde die Handlungsabsicht \enquote{\foreignlanguage{english}{To test the checkout process, simulate clicking on the 'Checkout' button in the cart overlay.}} (Deutsch: \enquote{Um den Bezahlvorgang zu testen, simuliere das Klicken auf den 'Checkout'-Knopf im Warenkorb-Overlay.}) generiert, obwohl das Warenkorb-Overlay nicht geöffnet war und somit der Knopf nicht existierte.
Die Eingabe verlangt aber, dass die Handlungsabsicht mit der Benutzerinteraktion ausgeführt werden soll, daher werden solche Handlungsabsichten ignoriert oder führen zu fehlerhaften Benutzerinteraktionen.

Um diese Probleme zu lösen, hatte ich die Idee, dass das Sprachmodell sich selbst Ziele setzen sollte, die es mit den nächsten Benutzerinteraktionen erreichen will.
Falls ein Ziel nicht mehr erreichbar scheint, sollte das Sprachmodell neue Ziele formulieren.
Gleichzeitig bekommt das Sprachmodell eine Auflistung der Ziele, die es sich zuvor gesetzt hat, und wie viele Benutzerinteraktionen es für jedes Ziel schon generiert hat.
Die Ergebnisse dieser Eingabe sind in Abbildung \ref{fig:gpt3_5_goals} zu sehen; diese Eingabe führte zu keiner relevanten Verbesserung der Ergebnisse.
Ein Hauptproblem war, dass das LLM fast bei jeder Benutzerinteraktion ein neues Ziel setzte, was dazu führte, dass es nie genug Benutzerinteraktionen für ein Ziel generierte, um es zu erreichen.

Die letzte Eingabe, die ich ausprobierte, war die \enquote{Chain of Thought}-Eingabe, bei der das Sprachmodell neben den, bei den anderen Eingaben erzeugten Gedankengängen, auch Beispiele für gute Gedankengänge und Benutzerinteraktion bekommt.
Diese Beispiele sind für andere Webseiten als die Testwebseite, um dem Sprachmodell keinen unfairen Vorteil gegenüber Monkey-Testing zu verschaffen.
Tatsächlich führte diese Eingabe zu einer Verbesserung der Ergebnisse, durchschnittlich erreichte das LLM damit eine Branchenabdeckung von $68{,}6$, wie in Abbildung \ref{fig:gpt3_5_chain_of_thought} zu sehen ist.
Der Unterschied der Branchenabdeckung zu Monkey-Testing ist relativ gering, und in einigen Fällen ist die Branchenabdeckung von GPT-3.5 höher als die von Monkey-Testing.

Insgesamt war das Ergebnis dieser Tests, dass die von mir definierten Eingaben zu einer Verbesserung der Ergebnisse führten, aber zumindest bei GPT-3.5 nicht ausreichten, um die Branchenabdeckung von Monkey-Testing zu übertreffen.
In einigen Tests hat das Sprachmodell den Bestellprozess, der ein Beispiel für eine für Monkey-Tester schwierige transaktionale Interaktion wie in Abbildung \ref{fig:sad_monkey} ist, teilweise durchgespielt.
Aber es wurde nie eine Bestellung abgeschlossen, während der Monkey-Tester ähnliche oder höhere Branchenabdeckung erreichte.
Es wurden auch keine Texteingaben generiert, die zu Validierungsfehlern führten, obwohl das Sprachmodell in der Eingabe dazu aufgefordert wurde, nach Randfällen zu suchen.


\section{Tests mit GPT-4}


\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            ymajorgrids=true,
            xmin=0, xmax=100,
            grid style=dashed,
            width=0.9\textwidth,
            ylabel={durchschnittliche Branchenabdeckung [\%]},
            xlabel={Anzahl der generierten Benutzerinteraktionen [1]},
            no markers,
            legend pos=south east,
            %xtick=data,
            ]
            \addplot[color=green] table [x=x,y=average,col sep=comma] {experimental_data/monkey/results.csv};
            \addlegendentry{Monkey-Testing, n=10}

            \addplot[color=black] table [x=x,y=average,col sep=comma] {experimental_data/gpt4-base/results.csv};
            \addlegendentry{GPT-4 Basiseingabe, n=1}

            \addplot[color=red] table [x=x,y=average,col sep=comma] {experimental_data/gpt4-chained/results.csv};
            \addlegendentry{GPT-4 Verkettung, n=2}

            \addplot[color=blue] table [x=x,y=average,col sep=comma] {experimental_data/gpt4-describe/results.csv};
            \addlegendentry{GPT-4 Beschreibung, n=3}

            \addplot[color=cyan] table [x=x,y=average,col sep=comma] {experimental_data/gpt4-explain/results.csv};
            \addlegendentry{GPT-4 Erklärung, n=5}

            \addplot[color=magenta] table [x=x,y=average,col sep=comma] {experimental_data/gpt4-goals/results.csv};
            \addlegendentry{GPT-4 Ziele, n=4}

            \addplot[color=orange] table [x=x,y=average,col sep=comma] {experimental_data/gpt4-chain-of-thought/results.csv};
            \addlegendentry{GPT-4 Chain of Thought, n=4}
        \end{axis}
    \end{tikzpicture}
    \caption{Vergleich der durchschnittlich gemessenen Branchenabdeckung von GPT-4 mit allen Eingaben und Monkey-Testing.}
    \label{fig:gpt4_all}
\end{figure}

\begin{figure}
    \centering
    \miniplot{gpt4-base}{GPT-4 Basiseingabe, n=1}{gpt4_base}{20}\hfill
    \miniplot{gpt4-chained}{GPT-4 Verkettung, n=2}{gpt4_chained}{100}
    \miniplot{gpt4-describe}{GPT-4 Beschreibung, n=3}{gpt4_describe}{100}\hfill
    \miniplot{gpt4-explain}{GPT-4 Erklärung, n=5}{gpt4_explain}{100}
    \miniplot{gpt4-goals}{GPT-4 Ziele, n=4}{gpt4_goals}{100}\hfill
    \miniplot{gpt4-chain-of-thought}{GPT-4 Chain of Thought, n=4}{gpt4_chain_of_thought}{100}
    \caption{Vergleich der durchschnittlich gemessenen Branchenabdeckung von GPT-4 (schwarz) und Monkey-Testing (grün). Der schattierte Bereich um die Linien zeigt die Standardabweichung.}
\end{figure}



\begin{figure}
    % 100 interactions
    \centering
    \begin{tabular}{|l|c|c|c|c|c|c|c|}
        \hline
        Modell & Eingabe & n & \multicolumn{4}{c|}{Branchenabdeckung [\%]} & Bestellung  \\
         &  &  & Mittel & Min & Max & $\sigma$ & abgeschlossen \\
        \hline
        Monkey & - & 10 & 81,5 & 74,5 & 84,7 & 3,2 & Nein \\
        GPT-4 & Verkettung & 2 & 83,2 & 80,6 & 85,7 & 2,6 & Ja \\
        GPT-4 & Beschreibung & 3 & 74,1 & 65,3 & 86,7 & 9,1 & Ja\\
        GPT-4 & Erklärung & 5 & 87,1 & 80,6 & 89,8 & 3.3 & Ja \\
        GPT-4 & Ziele & 4 & 86,0 & 78,6 & 89,8 & 4,6 & Ja \\
        GPT-4 & Chain of Thought & 4 & 85,7 & 82,7 & 87,8 & 1,9 & Ja \\
        \hline
    \end{tabular}
    \caption{Zusammenfassung der Ergebnisse der Tests mit GPT-4 und Monkey-Testing nach 100 Interaktionen.}
    \label{tab:results_100}
\end{figure}

Die Tests mit GPT-4 erreichten deutlich bessere Ergebnisse als die mit GPT-3.5.
Die Testläufe wurden außerdem mit 100 statt 25 Interaktionen durchgeführt, weil die Branchenabdeckung erst später zu stagnieren schien.
Wie auch mit GPT-3.5, führte die Basiseingabe zu unbenutzbaren Ergebnissen, wie in Abbildung \ref{fig:gpt4_base} zu sehen ist.
Deshalb habe ich die Tests mit dieser Eingabe nach 20 Interaktionen abgebrochen und die Eingabe nicht weiter getestet.
Bis auf die \enquote{Beschreibungs}-Eingabe führten alle anderen Eingaben im Durchschnitt zu einer höheren Branchenabdeckung als Monkey-Testing.
Weil das Sprachmodell GPT-4 relativ teuer ist, habe ich gerade bei den Eingaben, die wenig Verbesserung brachten, nur wenige Tests durchgeführt.

Schon die Verkettung der Eingaben führte zu einer deutlichen Verbesserung der Ergebnisse, wie in Abbildung \ref{fig:gpt4_chained} zu sehen ist.
Mit dieser Eingabe war die Branchenabdeckung nach 100 Interaktionen mit $83{,}2\%$, leicht höher als die von Monkey-Testing mit $81{,}5\%$.
Es ist allerdings zu beachten, dass bei den hohen Varianzen und der geringen Anzahl an Tests dieser Unterschied wenig aussagekräftig ist.
Mit dieser Eingabe wurde auch zum ersten Mal ein Bestellvorgang erfolgreich durchgespielt, jeweils mehrfach in beiden Testläufen.
Dabei wurden immer die Identitäten mit dem englischen Platzhalternamen \enquote{John Doe}, passenden E-Mail-Adressen und Addressen in New York verwendet.
Es wurden wieder keine Eingaben generiert, die zu Validierungsfehlern führten.

Die drei Tests mit der Beschreibungseingabe, zu sehen in Abbildung \ref{fig:gpt4_describe}, führten zu den, abgesehen von der Basis-Eingabe, zwei schlechtesten Branchenabdeckungen aller Testläufe mit GPT-4.
Das ist überraschend, weil die Beschreibungseingabe bei GPT-3.5 zu einer der besten Branchenabdeckungen führte.
Im schlechtesten dieser Tests war die Branchenabdeckung nach 100 Interaktionen nur $65{,}3\%$, das entspricht etwa der Abdeckung, die mit Monkey-Testing nach 15 Interaktionen erreicht wurde.
Die generierten Beschreibungen enthalten keine offensichtlichen Fehler, deshalb ist unklar, warum die Ergebnisse so schlecht sind.

Die Erklärungseingabe führte zu einer deutlichen Verbesserung der Ergebnisse, wie in Abbildung \ref{fig:gpt4_explain} zu sehen ist.
Mit dieser Eingabe war die durchschnittliche Branchenabdeckung nach 100 Interaktionen mit $87{,}1\%$, deutlich höher als die von Monkey-Testing mit $81{,}5\%$.
Dabei wurden mehrfach Eingaben generiert, die zu Validierungsfehlern führten, und auch der Bestellvorgang wurde mehrfach erfolgreich durchgespielt.
Die generierten Erklärungen und Beschreibungen lassen allerdings vermuten, dass das Sprachmodell versuchte, Eingaben zu generieren, die die Zahlungsmethode ändern, was auf der Testwebseite nicht möglich ist.
Dabei wurden scheinbar \enquote{versehentlich} Eingaben generiert, die zu den Validierungsfehlern und somit zu einer höheren Branchenabdeckung führten.

Die Zieleingabe führte zu Branchenabdeckungen, die nur geringfügig höher waren als die von Monkey-Testing, wie in Abbildung \ref{fig:gpt4_goals} zu sehen ist.
Wie mit GPT-3.5 setzte das Sprachmodell fast bei jeder Benutzerinteraktion ein neues Ziel, was zu einer langen Eingabesequenz führte, und wenig zielgerichteten Benutzerinteraktionen.

Die \enquote{Chain of Thought}-Eingabe führte zwar nicht zu einer höheren durchschnittlichen Branchenabdeckung als die Erklärungseingabe, aber zu einer geringeren Varianz, wie in Abbildung \ref{fig:gpt4_chain_of_thought} zu sehen ist.
Sie erreichte auch am schnellsten eine hohe Branchenabdeckung, wie in Abbildung \ref{fig:gpt4_all} zu sehen ist.
Während die Testabdeckung mit der Erklärungseingabe nach etwa 60-80 Interaktionen stagnierte, erreichte die Chain-of-Thought-Eingabe ihr Maximum $85{,}7\%$ in zwei Tests schon nach 33 Interaktionen.
In diesen Tests wurden keine Eingaben generiert, die zu Validierungsfehlern führten.

Insgesamt war das Ergebnis dieser Tests, dass die von mir definierten Eingaben zu einer Verbesserung der durchschnittlichen Branchenabdeckung führten.
Mit GPT-4 wurde die Branchenabdeckung von Monkey-Testing in den meisten Fällen erreicht oder übertroffen.
Wenn Validierungsfehler auftraten, dann weil das Sprachmodell versuchte, Eingaben zu generieren, die auf der Testwebseite nicht möglich sind.
Die daraus resultierenden Fehlermeldungen konnte das Sprachmodell dazu verwenden, um die Eingaben zu korrigieren und so die Bestellungen erfolgreich durchzuspielen.
