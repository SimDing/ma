%% LaTeX2e class for student theses
%% sections/content.tex
%% 
%% Karlsruhe Institute of Technology
%% Institute for Program Structures and Data Organization
%% Chair for Software Design and Quality (SDQ)
%%
%% Dr.-Ing. Erik Burger
%% burger@kit.edu
%%
%% Version 1.3.6, 2022-09-28

\chapter{Einleitung}
\label{ch:Introduction}

% testen ist wichtig

Das Testen von Software ist ein wichtiger Bestandteil der Softwareentwicklung, der sicherstellt, dass die Software wie erwartet funktioniert und die festgelegten Anforderungen erfüllt.
Es gibt verschiedene Arten von Softwaretests, darunter Unit-Tests, Systemtests und Akzeptanztests \cite{Sommerville10}.
Im Folgenden wird der Fokus auf funktionale Systemtests von Webanwendungen gelegt, die die gesamte Anwendung testen und dabei als Blackbox betrachten \cite{Beizer1990}.
Diese werden in Kapitel \ref{sec:Foundations:GUIBasedSystemTests} näher beschrieben.

% testen speziell von GUIs ist wichtig

Zumindest einzelne Komponenten einer Webanwendung werden gewöhnlich in dem Webbrowser des Benutzers ausgeführt, in dem die Anwendung auch dargestellt wird.
Dadurch ergeben sich besondere Herausforderungen für das Testen von Webanwendungen.
Einerseits verschärfen die Ausführung von Anwendungscode und Darstellung im Browser das Problem der Flakiness, bei der Tests kein konsistentes Ergebnis liefern, auch wenn die Anwendung nicht geändert wurde.
Gleichzeitig bedeuten Änderungen an der grafischen Benutzeroberfläche (Englisch: graphical user interface, GUI) oft, dass auch die Tests geändert werden müssen \cite{ChallengesSelenium}.
Lösungsansätze für diese Probleme sind Gegenstand aktiver Forschung und eine Auswahl wird in Kapitel \ref{ch:RelatedWork} vorgestellt.

% es wäre schön, wenn Tests ohne hartkodierte Fehlerbedingungen Fehler erkennen könnten

Diese Arbeit schließt an die Arbeit von \Citeauthor{GPT3Testing} an.
Ihr Ansatz ermöglicht das automatische Testen über GUIs, was an einer einfachen Webanwendung evaluieren.
Sie verwenden ein großes Sprachmodell (Englisch: large language model, LLM), um Benutzerinteraktionen zu generieren, mit denen die Anwendung getestet wird.
Dabei handelt es sich um eine Anwendung aus dem Bereich des maschinellen Lernens, die in der Lage ist, kontextuell angemessene Antworten auf eine Vielzahl von Aufgaben zu generieren \cite{FewShotLearners}.
Das LLM bekommt eine strukturierte Repräsentation der grafischen Benutzeroberfläche, vorherige Benutzerinteraktionen und eine natürlichsprachliche Testbeschreibung als Eingabe.
Es wird aufgefordert, eine Benutzerinteraktion zu generieren, die anschließend automatisch in der Anwendung ausgeführt wird \cite{GPT3Testing}.

In den Kapiteln \ref{ch:ExperimentalSetup} und \ref{ch:Results} liefert diese Arbeit zwei Beiträge:
Erstens wird der Ansatz an einer komplexeren Webanwendung mit verschiedenen Möglichkeiten zur Interaktion getestet.
Zweitens wird untersucht, ob das LLM gleichzeitig als Testorakel verwendet werden kann, das heißt, ob das LLM auch in der Lage ist, Fehlerzustände zu erkennen.

Absatz über \ref{ch:Conclusion}

Zuletzt befindet sich in Anlage \ref{ch:PracticalNotes} eine Sammlung von Erkenntnissen, die sich im Verlauf dieser Arbeit gezeigt haben und die für den praktischen Einsatz hilfreich sein könnten.


% LLMs sind vielversprechend \cite{GPT3Testing}, 


\chapter{Stand der Technik}
\label{ch:RelatedWork}

Ein verbreiteter Ansatz für das automatische Testen von Webanwendungen ohne hartkodierte Fehlerbedingungen ist das sogenannte Monkey-Testing.
Dabei werden zufällige Benutzerinteraktionen simuliert und beobachtet, ob die Anwendung abstürzt.
Dieser Ansatz ist einfach zu implementieren und kann auf jede Webanwendung angewendet werden, unabhängig von ihrer Struktur \cite{monkey_testing}.

Aufbauend auf Monkey Testing gibt es viele Ansätze mit der Idee Algorithmen zu entwickeln, die die zu Testende Anwendung effizienter erkunden.
TESTAR, ein Werkzeug für Monkey-Testing, enthält auch einen Algorithmus, der den Suchraum auf „interessante“ Pfade einschränken soll.
Dafür wird eine anwendungsspezifische Prolog Wissendatenbank verwendet die relevante Benutzeraktionen aus dem Anwendungszustand ableitet.
Aus diesen wird mittels Q-Learning ein Modell trainiert, das alle Anwendungszustände effizient zu erforschen versucht \cite{testar-q-learning}.
Eskonen et al. verwenden bildbasiertes Deep Reinforcement Learning, um zu imitieren, wie Menschen die zu testende Anwendung erkunden \cite{deep_reinforcement_exploring}.
Auf ähnliche Weise nutzen Saber et al. Computer Vision, um Anwendungszustände und Interaktionselemente aus Screenshots zu extrahieren, und verwenden dann Deep Q-Learning, um alle Anwendungszustände effizient zu erforschen \cite{saber_testing}. 
%Diese Ansätze machen zwar menschliche Interaktion überflüssig und sind auf jede GUI-basierte Anwendung anwendbar, ermöglichen aber nicht die automatische Abarbeitung von natürlichsprachlichen Designer-Testfällen.
Harris et al. schlagen DRIFT vor, ein Framework zur Navigation durch eine zu testende Anwendung bei der zuvor spezifizierte Softwarefunktionen ausgelöst werden sollen \cite{harries2020drift}.
DRIFT arbeitet mit einer symbolischen Darstellung der Benutzeroberfläche und verwendet Q-Learning mit Graph Neural Networks.
Dieser Prozess setzt jedoch die Beobachtung von Funktionsaufrufen der zu testenden Anwendung voraus, und die Definition von Tests erfordert technische Kenntnisse über die Interna der Anwendung.
Khaliq et al. verwenden ein ähnliches System wie das in dieser Arbeit vorgeschlagene: Sie übergeben kurze textuelle Beschreibungen der Benutzeroberfläche an ein LLM, das eine Sequenz von Benutzeraktionen generiert, die von diesem Zustand aus zu verfolgen sind \cite{transformers_exploratory}.
Neuere LLMs mit längereren maximalen Eingaben erlauben es jetzt aber, die vollständige Repräsentation der GUI in der Eingabe zu verwenden.

Diese Arbeit folgt dem Ansatz von \Citeauthor{GPT3Testing}, eine strukturierte Darstellung der Benutzeroberfläche der zu testenden Anwendung zusammen mit einer natürlichsprachlichen Testbeschreibung als Eingabe für ein LLM-Modell zu verwenden \cite{GPT3Testing}.
Das LLM generiert dann eine Benutzerinteraktion, die automatisch in der Anwendung ausgeführt wird.
Sie evaluieren ihren Ansatz anhand einer sehr einfachen Anwendung, die aus einer Ansicht mit Knöpfen besteht und einer Vielzahl von Testbeschreibungen.
Diese Arbeit zielt darauf ab, eine automatisierte Pipeline zu entwickeln und zu validieren, die den gleichen Ansatz für allgemeine GUI-basierte Webanwendungen mit HTML verwendet.

Unseres Wissens nach gibt es keine anderen Arbeiten, die LLMs für das automatisierte Testen von Webanwendungen verwenden.
In neueren Literaturübersichten wird die Linguistische Sprachverarbeitung zwar als Werkzeug für das Testen von Software aufgeführt, jedoch nicht für das Generieren von Benutzerinteraktionen \cite{implementation_verma_2023, machine_fontes_2021}.

Das hier in anderes Kapitel????
Scheinbar grundlegend für diese Anwendung ist die Fähigkeit des LLMs, die GUI und Zustände der zu testenden Anwendung zu verstehen und zu navigieren.
Zu dieser Navigation ist die Repräsentation der GUI als HTML-Dokument, einer Baumstruktur, von Bedeutung, und die Graphstruktur, die sich aus den Verknüpfungen der einzelnen Seiten und Zuständen der Anwendung ergibt.
Ergebnisse der Kognitionswissenschaft legen allerdings nahe, dass zum Verständnis solcher Graphstrukturen eine Art von räumlichem Verständnis erforderlich ist \cite{what_is_a_cognitive_map}, das bisher bei LLMs scheinbar nicht vorhanden ist \cite{cogmaps_llm}.
Erschwerend kommt hinzu, dass der Zustandsgraph der Webanwendungen in dieser Anwendung nicht explizit gegeben ist, sondern vom LLM aus den Interaktionen mit der Webanwendung abgeleitet werden müsste.

Einen komplett anderen Ansatz bieten Tsung et al.
Sie stellen einen Arbeitsablauf vor, der auf der Spezifikation von Testskripten in einer strukturierten, für den Menschen lesbaren Sprache basiert, die auch Bilder enthält \cite{tsung}.
Diese Skripte sind mit sehr wenig technischem Wissen verständlich und müssen nicht geändert werden, solange die visuellen Elemente der Benutzeroberfläche gleich bleiben.
Auch wenn keine natürliche Sprache verwendet wird und Änderungen an der Benutzeroberfläche immer noch Änderungen am Skript erforderlich machen können, werden einige Nachteile regulärer Testskripte beseitigt.

\chapter{Grundlagen}
\label{ch:Foundations}

\section{Systemtests auf Grundlage von grafischen Benutzeroberflächen}
\label{sec:Foundations:GUIBasedSystemTests}
Softwaretests sind ein wesentlicher Bestandteil der Softwareentwicklung.
Er umfasst die Bewertung einer Softwareanwendung oder eines Systems, um etwaige Mängel oder Fehler zu erkennen, die die Funktionalität, Zuverlässigkeit oder Leistung beeinträchtigen könnten.
Das Ziel von Softwaretests ist es, sicherzustellen, dass die Software wie erwartet funktioniert und die festgelegten Anforderungen erfüllt.
Es gibt verschiedene Arten von Softwaretests, darunter Unit-Tests, Integrations-Tests, Systemtests und Akzeptanztests.
Das Testen eines GUI-basierten Systems, bei dem nur auf die GUI zugegriffen wird, nennen wir einen GUI-basierten Systemtest oder GUI-Test.
Es wird oft von Software-Testern oder Qualitätssicherungsexperten durchgeführt, die eine Kombination aus manuellen und automatisierten Testmethoden benutzen, um Fehler zu erkennen und sicherzustellen, dass die Software die gewünschten Qualitätsstandards erfüllt.
Manuelle Tests sind ein gängiger Ansatz für GUI-basierte Systemtests, bei denen Tester Testfälle manuell ausführen, Benutzeraktionen simulieren und Beobachtungen und Feedback aufzeichnen.
Es können auch automatisierte Testtools wie Selenium und Appium verwendet werden, um GUI-Tests zu automatisieren, wodurch die Effizienz und Genauigkeit des Testprozesses erhöht werden kann.

GUI-Tests können grob in exploratives Testen und Testen auf der Grundlage von Designer-Testfällen unterteilt werden.
Beim explorativen Testen besteht das Ziel in der Regel darin, Pfade in der zu testenden Anwendung (Englisch: application under test, AUT) zu finden, die zu Abstürzen führen.
Eine grundlegende Technik für exploratives Testen, die leicht automatisiert werden kann, ist das Monkey-Testing, bei dem zufällige Benutzeraktionen simuliert werden.
Im Gegensatz dazu sind Tests, die von Designern spezifiziert werden, oft sehr viel kostspieliger.
Um diese zu automatisieren, müssen die Programmierer aus der Spezifikation ein Testskripte ableiten, die parallel zu Änderungen an der grafischen Benutzeroberfläche gewartet werden müssen.
Dies ist ein Problem, da die grafische Benutzeroberfläche oft der sich am häufigsten ändernde Teil eines Systems ist.

\section{Transformer: Ein Überblick}
\label{subsec:Foundations:Transformer}
Transformer \cite{transformers} sind eine Klasse von Deep-Learning-Modellen, die sich bei einer Vielzahl von Aufgaben der Linguistischen Datenverarbeitung wie maschineller Übersetzung, Texterzeugung und Sprachmodellierung als äußerst effektiv erwiesen hat \cite{transformers,bert,FewShotLearners,gpt4}.
Sie basieren auf einer Architektur, die sich von herkömmlichen rekurrenten neuronalen Netzen (RNNs) und Convolutional Neural Networks (CNNs) unterscheidet.
In jedem Schritt der Verarbeitung wird die Eingabesequenz zu jeweils einem Ausgabetoken verarbeitet.
Durch wiederholte Anwendung kann so eine Sequenz von Ausgabetoken erzeugt werden.
Ein grundlegender Baustein eines Transformator-Modells ist der Self-Attention-Mechanismus, der es dem Modell ermöglicht, sich während der Kodierungs- und Dekodierungsphasen auf bestimmte Teile der Ein- und Ausgabesequenzen zu „konzentrieren“.
Er ist notwendig um Abhängigkeiten zwischen weit voneinander entfernten Teilen der Eingabe zu erfassen, was besonders wichtig für Aufgaben des Sprachverständnisses ist.
In einem Transformer-Modell wird die Eingabesequenz zunächst von einer Reihe von Kodierungsschichten verarbeitet, die jeweils aus einem Self-Attention-Modul und einem neuronalen Feedforward-Netzwerk bestehen.
Das Self-Attention-Modul berechnet eine Gewichtung der Aufmerksamkeit auf die Positionen in der Eingangssequenz, die verwendet wird, um den Einfluss dieser Eingabeteile auf das jeweils nächste Ausgabetoken zu vergrößern.
Nach den Kodierungsschichten wird die Ausgabesequenz durch eine Reihe von Dekodierungsschichten erzeugt, die im Gegensatz zu den Kodierungsschichten sowohl auf die Eingabesequenz als auch auf die zuvor erzeugten Ausgabesequenzen zugreifen.
Die Dekodierungsschichten ermöglichen es dem Modell, Ausgabesequenzen zu erzeugen, die von der Eingabesequenz und zuvor erzeugten Ausgabetokens abhängen.
Einer der Hauptunterschiede von Transformermodellen gegenüber den älteren RNNs ist, dass zwischen den einzelnen Berechnungsschritten kein Zustand mitgeführt wird.
Das hat zwei Vorteile: Dieser Zustand nicht „vergessen“ werden und da keine Abhängigkeiten zwischen den Trainingsschritten existieren kann das Modell besser parallel trainiert werden.

Insgesamt können Transformermodelle Eingaben und Ausgaben variabler Länge verarbeiten und lassen sich mit großen Datensätzen trainieren, wodurch sie sich gut für Aufgaben der Linguistischen Datenverarbeitung eignen.
Transformer haben sich für eine Vielzahl von Aufgaben der Linguistischen Datenverarbeitung zum State-of-the-Art-Ansatz entwickelt und sind nach wie vor ein aktives Forschungsgebiet.
Zu den jüngeren Fortschritten gehören Techniken für das Training immer größerer Transformermodelle auf riesigen Datensätzen:
Zu den bekanntesten Transformermodellen gehören BERT \cite{bert}, GPT-3 \cite{FewShotLearners} und GPT-4 \cite{gpt4}, die sich vor allem durch ihre Größe, aber nicht grundlegend durch unterschiedliche Architekturen unterscheiden \footnote{Im Fall von GPT-4, werden auch Bilder als Teil der Eingabe akzeptiert. Die genaue Architektur ist nicht öffentlich bekannt.}.
Als Bezeichnung für diese Modelle hat sich der Begriff Large Language Model (LLM) etabliert.

\section{Große Sprachmodelle und ihre Anwendungen in der Softwareentwicklung}
\label{subsec:Foundations:LLM}

Large language models like GPT-3 can be used task-agnostically because they are trained on vast amounts of text data in an unsupervised manner. During training, the model learns to understand the underlying patterns and relationships in language, allowing it to generate coherent and contextually appropriate responses to a wide range of tasks without the need for task-specific training data.
One key advantage of these models is their massive size, with GPT-3 containing 175 billion parameters.
This allows them to capture a vast amount of linguistic knowledge and context.
Because these models have been trained on such a large and diverse corpus of text, they are able to learn and generalize from patterns and relationships that are common across many different types of language use. This allows them to perform well on a wide variety of natural language processing tasks, such as language translation, sentiment analysis, and question answering, among others.
The sheer number of parameters used in these models allows them to capture a wide range of linguistic features, from simple grammatical structures to more complex semantic relationships. By leveraging this large amount of training data and parameter space, these models are able to learn to represent language in a highly nuanced and contextually appropriate way, allowing them to perform well on a wide range of tasks without the need for additional training.
Furthermore, GPT-3 has shown impressive capabilities in generating source code for computer programs, including JavaScript, Python, and SQL. As a language model trained on a massive amount of code and natural language data, ChatGPT can also generate source code with a high degree of accuracy and efficiency, making it a valuable tool for software development and automation tasks.
As such LLMs appear to be adequate to process both the structured representation of an application’s GUI, as well as natural language test cases, and provide a description of what action to follow next.


\chapter{Versuchsaufbau}
\label{ch:ExperimentalSetup}

The idea of this thesis is to implement an automated system for GUI-based software testing that uses language models to systematically test systems following predefined natural-language test descriptions.
The natural language model will be prompted with a symbolic representation of the GUI’s state as well as the test description and asked to generate a user input to execute the test.
It will then simulate the user input in the AUT and repeat.
It will also be asked whether errors have occurred during the test execution.
The same setup will be evaluated with different test descriptions and language models.

\section{Forschungsfragen}

\begin{enumerate}
    \item Can LLMs understand HTML?
    \item Feasibility of the approach
    \item Do you need the whole document? / Advantage of “stripped tree”
    \item Flakiness > Repeats necessary / how many?
    \item Optional: Possibility and Accuracy of validation
    \item Optional: Impact of prompt engineering?
\end{enumerate}

\section{Getestete Anwendung}

As part of this thesis, a custom demo application will serve as the application under test.
This demo application will mimic a real-world web shop application, where users can interact with various elements such as product listings, shopping carts, checkout processes, and user profiles.
This will allow testing the approach under realistic circumstances with diverse GUI elements, transactional workflows, and data validation.
To save time, the demo application will be implemented using an open-source webshop template, which will be modified to include additional features and functionality for validation.
Multiple open-source web shop projects have been screened for their suitability for this purpose as described in table \ref{tab:webshop_projects}.
Of these templates, EverShop has been chosen as the basis for the demo application, as it is the most actively maintained, is intended to be used commercially, and is licensed under the GNU General Public License, version 3.

\begin{figure}[h]
    \label{tab:webshop_projects}
    \begin{minipage}[c]{\textwidth}
        \centering
        {
            \scriptsize
            \begin{tabular}{ | l | l | l | l | l |}
                \hline
                \textbf{Template} & \textbf{Last Commit} & \textbf{License} & \textbf{Technology} & \textbf{Remark} \\ \hline
                Online Shopping Cart\footnote{\url{https://github.com/shashirajraja/shopping-cart}} & May 23 & Apache 2 & Java, Servlets, JSP, SQL & Demonstrational \\ \hline
                LifestyleStore\footnote{\url{https://github.com/sajalagrawal/LifestyleStore}} & Aug 20 & - & PHP, SQL & learning purpose \\ \hline
                Your Online Shop\footnote{\url{https://github.com/petazeta/youronlineshop}} & Dec 22 & custom & Node.js, MongoDB & - \\ \hline
                Ecommerce-Website\footnote{\url{https://github.com/winston-dsouza/ecommerce-website}} & Feb 20 & MIT & PHP, SQL & no checkout / registration \\ \hline
                Book-Hub\footnote{\url{https://github.com/amberkakkar01/Book-Hub}} & Aug 21 & - & HTML & static pages \\ \hline
                online-shopping-site\footnote{\url{https://github.com/dinushchathurya/online-shopping-site}} & May 19 & GPLv3 & JavaScript & no server state \\ \hline
                Online-Fashion-Store\footnote{\url{https://github.com/RazaRizvii/Online-Fashion-Store}} & Dec 22 & - & JavaScript & no server state \\ \hline
                online-shopping-website\footnote{\url{https://github.com/nsk1512/online-shopping-website}} & Dec 19 & - & Node.js, MongoDB & - \\ \hline
                Ketra-Mart\footnote{\url{https://github.com/Prajwal100/ketra-mart-free}} & Jun 23 & MIT & ? & source code against payment \\ \hline
                Shuup\footnote{\url{https://github.com/shuup/shuup}} & Aug 21 & OSL & Python, SQL & Commercial, Multi-Vendor \\ \hline
                EverShop\footnote{\url{https://github.com/evershopcommerce/evershop}} & Sep 23 & GPLv3 & Node.js, SQL & - \\ \hline
                Hayroo\footnote{\url{https://github.com/hasan-py/MERN_Stack_Project_Ecommerce_Hayroo}} & May 23 & - & Node.js, MongoDB & - \\ \hline
            \end{tabular}
        }
    \end{minipage}
    \caption{Open-source web shop projects that were screened for suitability for this thesis}
\end{figure}

\section{Validierung}

As we know of no other approaches to doing scripted testing using only natural language test descriptions, comparing it to other works is hard.
As such validation of this part of the work will center around its feasibility.
The new system's ability to navigate the AUT will be compared to monkey testing.
The test oracle part of the system will be validated using mutation testing and its results will be captured in a confusion matrix.

\iffalse
\chapter{Execution}

\section{Riscs}

\section{Schedule}

\ganttset{%
    calendar week text={%
        \currentweek
    }%
}
\begin{ganttchart}[hgrid,
time slot format=isodate,
    x unit=2pt,]{2023-06-12}{2023-12-11}
    \gantttitlecalendar{year, month, week}\\
    %\gantttitle{2023-06-12}{2024-01-1} \\
    %\gantttitlelist{1,...,12}{1} \\
    \ganttbar{Lit. Research}{2023-06-12}{2023-06-26} \\%2
    \ganttgroup{Implementation}{2023-06-26}{2023-08-21} \\
    \ganttbar{AUT}{2023-06-26}{2023-07-24} \\%4
    \ganttbar{Implementation}{2023-07-24}{2023-08-21} \\%4
    \ganttgroup{Validation}{2023-08-21}{2023-10-02} \\
    \ganttbar{Design Tests}{2023-08-21}{2023-09-04} \\%2
    \ganttbar{GPT3}{2023-09-04}{2023-09-11} \\%1
    \ganttbar{GPT4}{2023-09-11}{2023-09-18} \\%1
    \ganttbar{Monkey Testing}{2023-09-18}{2023-10-02} \\%2
    %\ganttbar{open source GPT}{2023-10-09}{2023-10-23} \\
    %\ganttbar{Monkey Testing}{2023-10-09}{2023-10-23} \\
    \ganttgroup{Documentation}{2023-10-02}{2023-11-27} \\%4
    \ganttbar{Writing}{2023-10-02}{2023-11-13} \\%6
    \ganttbar{Finalize}{2023-11-13}{2023-11-27} \\%2
    \ganttbar{Buffer}{2023-11-27}{2023-12-11}%2
    %\ganttbar{Final Task}{8}{12}
    %\ganttlink{elem2}{elem3}
    %\ganttlink{elem3}{elem4}
\end{ganttchart}
\fi

%% ---------------------
%% | / Example content |
%% ---------------------